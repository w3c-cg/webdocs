# W3C Docs CG Conversational Docs session at TPAC, Kobe, Japan, 2025-11-10

Present: Dom, FLorian, WIllB, Estelle, Francois, Roy, Alan Buxey, Nour Nabil, Takaaka Nishioka, Simeon Vincent, Mike Smith, Richard Ishida, Francois, Kadir Topal, Andrew Liu, Michael Pennisi, Dan Appelquist

Florian: with AI, more devs are in sync AI conversations than reading or in Stack Overflow threads. Also more info appears in autocomplete or search engine results.

Florian: less traffic in traditional knowledge sites like Wikipedia or MDN. Contextualized situational interactions instead.

Florian: SO is a lot like an AI, in that it promises to answer very specific questions. But results are publicly visible and could be improved by other contributors, and was present as a common good. That's now a private experience, others don't get to see the answers (or improve them). MDN is a bit different but still sees a big effect.

Florian: more content != more knowledge. AI must RTFM on behalf of its users. We have no insight into the context users have when they use AI, harder to provide tailored documentation for them.

Florian: still "classical" writers but also "context curators" providing situational pointers: best practices, checklists, recommendations.

Dom: risk of platform being frozen in the past due to training latency: Dead framework theory (Paul Kinlan). Risk of bad/old practices being reinforced - because training is statistical, so popular wins. So need to find the right way to feed best practices into coding agents. Could even use agents to push adoption of technologies to use the best practices.

Dom: if we consider AI systems as a new target for documentation, with the theory that we can guide them towards a better outcome, then we need to understand their own limitations. For example, they have a very limited context window: the amount of info you can give them before they start hallucinating. How much do we want to invest in LLMs as a customer of technical docs? If we think they are customers, how do we adapt what we do to serve them effectively? Not just how we do it but how to deploy it in the real world.

Alan: LLMs text proposal: the issue with LLMs is the context window. LLMs text is docs for AIs.

Mike: is this something that is produced in parallel? As well as docs for humans?

Alan: in parallel.

Mike: why aren't the machines smart enough to understand the human-version? Isn't that the point?

Alan: partly context window. You don't need all the other documentation. AI doesn't need informative text, only normative text. Can be less friendly, not include the stuff for humans.

Estelle: if it only reads normative text, why not read specs?

Alan: depends how the docs have been written.

Dom: one of the values of LLM.text is that LLMs don't benefit from navigation or fluff. But we have seen problems whenever we've tried duplicating content. LLM.text addresses a narrow part of the problem. It doesn't address: how does an LLM get to the right answer. If we ask a chatbot to do something, how can we ensure it will use the techniques we now recommend.

Vadim: we did research on LLMs in MDN. LLM.text is really just to make the content smaller. Currently they don't consume whole docs if they are too big. Context window might get bigger, this could be a temporary problem. No headers, footers, sidebars.

Simeon: this seems like a model training problem, that providers should be souring accurate up to date data. Is this about trying to feed agentic systems supplementary data?

Florian: when you ask an SO question there is a discussion where experts figure out the best solution. with LLMs the answer depends on you and what exactly you ask.

Dom: LLMs are trained on a vast corpus which isn't optimised for web dev best practices. If we can provide a system where a coding agent can get access to best practices, then they get better outcomes. Training phase is not the right time because it is too much in the past, and it is entirely in the hands of whoever it training it. We can't influence that. Whereas systems like MCP servers we can provide input.

Kadir: attaching docs to web feature IDs to scope them for an MCP server? Only need the docs that pertain to the thing you want to write. Web feature IDs as a way to scope that. There is a danger of LLMs using outdated patterns.

Dom: say, if an LLM knows you're using a feature with an ID, it can get the right docs. But it can't tell the LLM about different web features that they don't know about yet.

Kadir: but the MCP server can say, check with web features for relevant features, that refer to the thing you are trying to do.

Vadim: like search engines. w3schools website tops search engines. What could we do? Nothing, because it's the search engine's internal algorithm, we can't influence it. But modern LLMs are still approachable, can partner with them and say, we provide good docs, let's work together. This isn't possible with search engines.

Fran√ßois: LLMs move fast and it's hard to predict the future. But machines should be good at following links and understanding pointers to things, mapping from IDs to patterns that should be used.

Mike (Bocoup): skeptical about changing practices for machines, should trust intuitions. But Dom's suggestion about context and moving people towards newer practices makes it more compelling. Helping people use older models to not always need the latest model represents some degree of harm reduction.

Florian: a lot of people are using these things. These are people we used to talk to and we still want to be able to reach them.

Estelle: we should not stop what we are doing because we also need to be feeding the AI good information. So not too much time working out how to work with LLMs but also to keep writing good documentation.

Florian: but they are not going away.

Lola: important that LLMs have accurate data, not nonsense. Not so much worried about people using LLMs in work, because of review. More concerned about non-engineers who want to build something for their business or side project, use LLMs and do not understand the code at all. These people are at higher risk of bad consequences (as are their users). Is this an ongoing thing? To Estelle's point, we don't want to be continually feeding the LLMs, how do we balance the work of writing tech docs?

Florian: we are still writing technical documentation now. What you said related to context curation. Beginners have less context, more experienced people can ask better questions and get better results. How can we provide context to beginners?

Vadim: speaking of hand-chiselled words: in this era of LLM content on the web it is important to keep everything human curated and have LLM policies for content. So we can give users guaranteed human curated content.

Kadir: what goes into MDN still needs to be reviewed by humans. The source does not matter so much.

Will: but this is a problem for reviewers swamped by poor quality PRs.

Dom: IPR is also an issue for LLM generated PRs.

Florian: the legal community is catching up on this and it will likely become more of an issue.

Estelle: asking AI about new stuff, it is wrong because the stuff is new and there is nothing to read.

Kadir: for code samples, though, if you review them.

Dom: maybe for some trusted collaborators we might want to allow this but for too open-ended policies it is a lot of work for reviewers. Complete ban is excessive perhaps but controls are needed.

Estelle: LLMs don't use best practices.

Roy: first layer is documentation (W3C, MDN), second @@@, third is usage content (bots, agents), fourth is tooling (browsers, IDE). For documents, we can add metadata to improve them and how they relate one to another; 3rd layer, test with embeddings, prompts to guide LLM towards specs via vectors; 4th layer, maybe similar things could be done.

Mike: I don't want to put my time into making people more stupid and lazy. If this is what we are doing then we are lost, including myself. Normal use case for LLMs is to generate code. People just want to get something done, not to read the explanation. Very transactional information need. Including myself in this, it leads to not making me think for myself, and I don't want to do things that facilitate this. Do we want other humans to feed people information the way machines do. Stack Overflow "help vampire", just keeps asking questions about the next tiny thing that they want to do, and LLMs are a supercharged version of this. Do we want to make that worse. Can we make it better?

Florian: this is demoralising as a tech writer, you want to help someone learn something, understand a concept.

Estelle: people are still going to school, maybe the teacher finds it and reads it.

Dan: not if teacher use AI to grade homework and the students use it.

Dom: in agreement with these challenges. Maybe instead of the LLM giving you code it gives you a challenge, and gives you feedback on it. There are ways to use them that make you smarter not stupider. but if they have access to the wrong data they may make you learn the wrong things.

Mike: I don't want to do any extra work for that. LLM.text is the machine training us to do something for them.

Dom: autocomplete makes you lazier too. But noone cares. Challenging to know where the boundary between this and it making you stupid is.

Estelle: I learn from autocomplete because I can see what's available to me.

Mike: some people don't like IDEs for similar reasons. So yes, not black and white.

Lola: a study in the UK about AI use making us lazier has been debunked. So there isn't empirical evidence.

Mike: it has personally made me lazier.

Kadir: if the goal is to teach us things, I get it. But it's not about that, or even developers. It's just mediated through developers, the end goal is to provide users with an experience on the WP. There might be effects on other things.

Mike (Bocoup): think of times we've found solutions by being inefficient and human. So long as we decide that there are some beneficial applications, it's worth considering how to make it work better. It's the end user deciding how they are using the technology, whatever our intentions are.

Dom: you could ask, how much priority do we put in? I think there is a need here. Because if developers miss the best practices, we are failing our mission. But it is hard to know how much to prioritize it. It is a valid position to say, we don't have the resources to do this.

Alan: maybe also a case of awareness, rather than devs using old docs, their LLM is using the newer docs. Much less energy efficient.

Dom: yes, can we make it easy for agents to have access to the right information while being sustainable.

Mike: there's already been an erosion of funding for documentation. Adding more stuff on top of it seems unsustainable for the team's resources.

Florian: these systems are not giving back to content creators generally.

Dom: yes, so it might be possible to get more funding for documentation in this way.

Florian: description of problem space is useful even if we don't have answers yet.
